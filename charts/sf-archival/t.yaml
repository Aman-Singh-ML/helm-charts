---
# Source: sf-archival/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: archival-service-account
  namespace: kafka
  labels:
    release: archival
    snappyflow/projectname: snappyflow-app
    snappyflow/appname: archival
---
# Source: sf-archival/templates/cloud-specific.yaml
apiVersion: v1
data:
  key: WFhYWA==
  secret: WFhYWA==
kind: Secret
metadata:
  name: archival-aws-secret
  namespace: kafka
  labels:
    chart: sf-archival
    release: archival
    heritage: Helm
type: Opaque
---
# Source: sf-archival/charts/compaction-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-compaction-controller
  namespace: kafka
data:
  config.json: |-
    {
      "compactionURL": "http://archival-compaction-controller",
      "datasetControllerURL": "http://archival-dataset-controller",
      "datasetRawControllerURL": "http://archival-dataset-raw-controller",
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 1,
        "max_idle_connections": 0
      },
      "jobServer": "http://archival-spark-manager",
      "serverPort": ":5005",
      "ingestControllerURL": "http://archival-ingest-controller",
      "maxRunning": 1,
      "maxAttempts": 2,
      "pendingStateTolerationHours": 1,
      "runningStateTolerationHours": 3
    }
---
# Source: sf-archival/charts/dataset-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-dataset-controller
  namespace: kafka
data:
  config.json: |-
    {
      "ingestControllerURL": "http://archival-ingest-controller",
      "rawQueryExecControllerURL": "http://archival-raw-query-exec-controller",
      "datasetRawControllerURL": "http://archival-dataset-raw-controller",
      "logArchivalURL": "http://archival-log-archival",
      "billingURL": "https://127.0.0.1:8000/api/v1/records/",
      "rawDataCleanupDays": 2,
      "maxGiBToAnalyzeConcurrently": 1,
      "presto": {
        "user": "ubuntu",
        "catalog": "hive",
        "schema": "default"
      },
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 2,
        "max_idle_connections": 1
      },
      "serverPort": ":5001"
    }
---
# Source: sf-archival/charts/dataset-raw-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-dataset-raw-controller
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 3,
        "max_idle_connections": 1
      },
      "serverPort": ":5002",
      "final_segment_size_gb": 1,
      "final_segment_size_buffer_pcnt": 15,
      "num_orcs_in_segment": 4,
      "num_backtrack_days": 5,
      "ingest_controller_url": "http://archival-ingest-controller",
      "dataset_controller_url": "http://archival-dataset-controller",
      "bucket_partition_duration_mins": 15,
      "event_queue_handler_monitor_interval_in_sec": 2,
      "orc_inventory_batch_fetch_size": 20000,
      "max_buffered_events": 5000,
      "competingConsumers": 10
    }
---
# Source: sf-archival/charts/hive-and-hadoop/templates/env-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: archival-hive-and-hadoop-env
  labels:
    chart: hive-and-hadoop
    release: archival
    heritage: Helm
data:
  HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://host:5432/hive_metastore?useSSL=false"
  HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
  HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: archive
  HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: archive123
  HIVE_SITE_CONF_datanucleus_autoCreateSchema: "false"
  HIVE_SITE_CONF_hive_metastore_uris: thrift://archival-hive-metastore:9083
  HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check: "false"
  CORE_CONF_fs_defaultFS: hdfs://archival-namenode-headless:8020
  CORE_CONF_hadoop_http_staticuser_user: root
  CORE_CONF_hadoop_proxyuser_hue_hosts: "*"
  CORE_CONF_hadoop_proxyuser_hue_groups: "*"
  HDFS_CONF_dfs_webhdfs_enabled: "true"
  HDFS_CONF_dfs_permissions_enabled: "false"
  YARN_CONF_yarn_log___aggregation___enable: "false"
  YARN_CONF_yarn_resourcemanager_recovery_enabled: "false"
  YARN_CONF_yarn_resourcemanager_store_class: org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
  YARN_CONF_yarn_resourcemanager_fs_state___store_uri: /rmstate
  YARN_CONF_yarn_nodemanager_remote___app___log___dir: /app-logs
  YARN_CONF_yarn_log_server_url: http://historyserver:8188/applicationhistory/logs/
  YARN_CONF_yarn_timeline___service_enabled: "false"
  YARN_CONF_yarn_timeline___service_generic___application___history_enabled: "false"
  YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled: "false"
  YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
  YARN_CONF_yarn_timeline___service_hostname: historyserver
  YARN_CONF_yarn_resourcemanager_address: resourcemanager:8032
  YARN_CONF_yarn_resourcemanager_scheduler_address: resourcemanager:8030
  YARN_CONF_yarn_resourcemanager_resource__tracker_address: resourcemanager:8031
---
# Source: sf-archival/charts/ingest-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-ingest-controller
  namespace: kafka
data:
  config.json: |-
    {
      "datasetURL": "http://archival-dataset-controller",
      "datasetRawControllerURL": "http://archival-dataset-raw-controller",
      "queryControlURL": "http://archival-query-controller",
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 3,
        "max_idle_connections": 1
      },
      "compactPartitionColumnNames": ["year", "month", "day", "segment"],
      "rawPartitionColumnNames": ["year", "month", "day", "timebucket"],
      "hive": {
        "userName": "root",
        "host": "archival-hive-server",
        "port": 10000,
        "auth": "NONE"
      },
      "logArchivalURL": "http://archival-log-archival",
      "schemaRegistryURL": "http://sf-datapath-cp-schema-registry:8081",
      "kafkaConnectURL": "http://sf-datapath-archival-kafka-connect:8083",
      "kafkaBrokers": "localhost:9092,localhost1:9092",
      "patternsURL": "http://sf-datapath-signatures-and-kafka-apis",
      "serverPort": ":5000"
    }
  sink_defaults.json: |-
    {
      "max_tasks_per_topic": 3,
      "flush.size": 10000,
      "partitioner.class": "io.confluent.connect.storage.partitioner.SfFieldTimeBucketPartitioner",
      "path.format": "'year'=YYYY/'month'=M/'day'=d/'time-bucket'='TIME-BUCKET'",
      "locale": "US",
      "timezone": "UTC",
      "partition.field.name": "_tag_projectName, _tag_appName, _plugin, _documentType",
      "timestamp.extractor": "RecordField",
      "timestamp.field": "time",
      "schema.compatibility": "NONE",
      "rotate.schedule.interval.ms": 180000,
      "partition.duration.ms": 900000,
      "transforms": "filter,insertsignature",
      "transforms.filter.type": "com.logarchival.MarkDrop$Value",
      "transforms.insertsignature.type": "com.signature.InsertSignature$Value",
      "transforms.insertsignature.connectorname.field": "archiver",
      "errors.tolerance": "all",
      "sf.archival.schema.keys": "_tag_projectName, _tag_appName, _plugin, _documentType",
      "behavior.on.null.values": "ignore",
      "connector.class": "io.confluent.connect.s3.S3SinkConnector",
      "s3.part.size": 5242880,
      "storage.class": "io.confluent.connect.s3.storage.S3Storage",
      "format.class": "io.confluent.connect.s3.format.orc.OrcFormat",
      "store.url": "s3.amazonaws.com"
    }
---
# Source: sf-archival/charts/log-archival/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-log-archival
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 1,
        "max_idle_connections": 0
      },
      "serverPort": ":5003",
      "kafkaBrokers": "localhost:9092,localhost1:9092"
    }
---
# Source: sf-archival/charts/query-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-query-controller
  namespace: kafka
data:
  config.json: |-
    {
      "datasetControllerURL": "http://archival-dataset-controller",
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 1,
        "max_idle_connections": 0
      },
      "hive": {
        "userName": "root",
        "host": "archival-hive-server",
        "port": 10000,
        "auth": "NONE"
      },
      "ingestControllerURL": "http://archival-ingest-controller",
      "jobTypes": ["LogQuery", "Histogram"],
      "logArchivalURL": "http://archival-log-archival",
      "serverPort": ":5004",
      "maxGBToProcessDuringSplitQuery": 0.2,
      "queryParallelismFactor": 5,
      "steppedParallelism": false
    }
---
# Source: sf-archival/charts/query-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-query-controller-execution-controller
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 2,
        "max_idle_connections": 1
      },
      "presto": {
        "user": "maplelabs",
        "catalog": "hive",
        "schema": "default"
      },
      "logArchivalURL": "http://archival-log-archival",
      "interval": 250,
      "intervalUnit": "ms",
      "internal_hive": true,
      "maxAllowedQueriesPerJob": 5,
      "maxQueriesPerCluster": 15
    }
---
# Source: sf-archival/charts/query-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-query-controller-cleanup
  namespace: kafka
data:
  config.json: |-
    {
      "cleanIntervalInHour": 1,
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432
      },
      "queryControllerURL": "http://archival-query-controller"
    }
---
# Source: sf-archival/charts/raw-query-exec-controller/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-raw-query-exec-controller
  namespace: kafka
data:
  config.json: |-
    {
      "presto": {
        "user": "maplelabs",
        "catalog": "hive",
        "schema": "default"
      },
      "logArchivalURL": "http://archival-log-archival",
      "datasetControllerURL": "http://archival-dataset-controller",
      "ingestControllerURL": "http://archival-ingest-controller",
      "maxGBToProcessDuringHistogramQuery": 0.2,
      "maxGBToProcessDuringLogQuery": 0.05
    }
---
# Source: sf-archival/charts/spark-manager/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-spark-manager-monitor
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432
      }
    }
---
# Source: sf-archival/charts/spark-manager/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-spark-manager-jobserver
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432,
        "max_open_connections": 1,
        "max_idle_connections": 0
      },
      "serverPort": ":5002"
    }
---
# Source: sf-archival/charts/spark-manager/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-spark-manager-infrastructure
  namespace: kafka
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432
      },
      "infrastructures": [
        {
          "mode": "KUBERNETES",
          "preference": 10,
          "template": {
            "pod_namespace": "kafka",
            "command": [
              "/opt/spark-3.1.2-bin-hadoop3.2/bin/./spark-submit",
              "--name",
              "__PARAM__name",
              "--master",
              "k8s://kubernetes.default.svc:443",
              "--deploy-mode",
              "cluster",
              "--conf",
              "spark.kubernetes.driver.limit.cores=0.4",
              "--conf",
              "spark.kubernetes.executor.limit.cores=1",
              "--conf",
              "spark.executor.instances=1",
              "--conf",
              "spark.kubernetes.executor.request.cores=1",
              "--conf",
              "spark.kubernetes.driver.request.cores=0.4",
              "--conf",
              "spark.executor.memory=2048m",
              "--conf",
              "spark.driver.memory=1024m",
              "--conf",
              "spark.eventLog.enabled=true",
              "--conf",
              "spark.eventLog.dir=s3a://sparkhs/spark-hs",
              "--conf",
              "spark.kubernetes.driver.annotation.prometheus.io/scrape=false",
              "--conf",
              "spark.kubernetes.driver.label.snappyflow/projectname=snappyflow-app",
              "--conf",
              "spark.kubernetes.executor.label.snappyflow/projectname=snappyflow-app",
              "--conf",
              "spark.kubernetes.driver.label.snappyflow/appname=archival",
              "--conf",
              "spark.kubernetes.executor.label.snappyflow/appname=archival",
              "--conf",
              "spark.kubernetes.driver.label.release=archival",
              "--conf",
              "spark.kubernetes.executor.label.release=archival",
              "--conf",
              "spark.kubernetes.container.image=snappyflowml/spark-compact-orc:4",
              "--conf",
              "spark.kubernetes.container.image.pullSecrets=xxxx",
              "--conf",
              "spark.kubernetes.authenticate.driver.serviceAccountName=archival-service-account",
              "--conf",
              "spark.kubernetes.authenticate.executor.serviceAccountName=archival-service-account",
              "--conf",
              "spark.kubernetes.authenticate.serviceAccountName=archival-service-account",
              "--conf",
              "spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=archival-aws-secret:key",
              "--conf",
              "spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=archival-aws-secret:secret",
              "--conf",
              "spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=archival-aws-secret:key",
              "--conf",
              "spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=archival-aws-secret:secret",
              "--conf",
              "spark.kubernetes.namespace=kafka",
              "--class",
              "sf.org.hadoop.Compact",
              "local:///app/compact-orc-scala-2.12.10.jar",
              "__PARAM__source",
              "__PARAM__segmentFile",
              "__PARAM__destination"
            ]
          }
        }
      ]
    }
---
# Source: sf-archival/templates/cloud-specific.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: archival-cloud-env
  namespace: kafka
  labels:
    chart: sf-archival
    release: archival
    heritage: Helm
data:
  CLOUD_NAME: AWS
  AWS_QUEUE_URL: XXXX
  AWS_QUEUE_ARN: XXXX
  AWS_ACCESS_KEY_ID: XXXX
  AWS_SECRET_ACCESS_KEY: XXXX
  AWS_REGION: us-west-2
---
# Source: sf-archival/templates/system-migration.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: archival-system-migration-config
  namespace: kafka
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "archival"
    helm.sh/chart: "sf-archival-2.0.88"
data:
  config.json: |-
    {
      "db": {
        "name": "new_archival",
        "user": "archive",
        "password": "archive123",
        "host": "host",
        "port": 5432
      },
      "hive": {
        "userName": "root",
        "host": "archival-hive-server",
        "port": 10000,
        "auth": "NONE"
      },
      "compactPartitionColumnNames": ["year", "month", "day", "segment"],
      "rawPartitionColumnNames": ["year", "month", "day", "timebucket"],
      "datasetRawControllerURL": "http://archival-dataset-raw-controller"
    }
---
# Source: sf-archival/templates/list-eks-nodes.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: archival-list-eks-nodes
rules:
- apiGroups:
  - ""
  resources:
  - "nodes"
  verbs:
  - "list"
---
# Source: sf-archival/templates/cluster-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: archival-spark-role-default
  namespace: kafka
subjects:
- kind: ServiceAccount
  name: archival-service-account
  namespace: kafka
roleRef:
  kind: ClusterRole
  name: edit
  apiGroup: rbac.authorization.k8s.io
---
# Source: sf-archival/templates/list-eks-nodes.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: archival-list-eks-nodes-binding
subjects:
- kind: ServiceAccount
  name: archival-service-account
  namespace: kafka
roleRef:
  kind: ClusterRole
  name: archival-list-eks-nodes
  apiGroup: rbac.authorization.k8s.io
---
# Source: sf-archival/charts/compaction-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-compaction-controller
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5005
    protocol: TCP
  selector:
    app.kubernetes.io/name: compaction-controller
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/dataset-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-dataset-controller
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5001
    protocol: TCP
  selector:
    app.kubernetes.io/name: dataset-controller
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/dataset-raw-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-dataset-raw-controller
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5002
    protocol: TCP
  selector:
    app.kubernetes.io/name: dataset-raw-controller
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/hive-and-hadoop/templates/datanode-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-datanode-headless
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hadoop
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - port: 50010
      name: datanode
      protocol: TCP
  selector:
    app.kubernetes.io/name: hadoop-datanode
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/hive-and-hadoop/templates/hive-metastore-external-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-hive-metastore-external
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hive
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
  - port: 9083
    name: metastore-ext
    nodePort: 32500
    protocol: TCP
  selector:
    app.kubernetes.io/name: hive-metastore
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/hive-and-hadoop/templates/hive-metastore-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-hive-metastore
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hive
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - port: 9083
    name: metastore
    protocol: TCP
  selector:
    app.kubernetes.io/name: hive-metastore
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/hive-and-hadoop/templates/hive-server-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-hive-server
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hive
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - port: 10000
    name: server
    protocol: TCP
  selector:
    app.kubernetes.io/name: hive-server
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/hive-and-hadoop/templates/namenode-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-namenode-headless
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hadoop
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - port: 8020
      name: namenode
      protocol: TCP
  selector:
    app.kubernetes.io/name: hadoop-namenode
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/ingest-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-ingest-controller
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5000
    protocol: TCP
  selector:
    app.kubernetes.io/name: ingest-controller
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/log-archival/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-log-archival
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5003
    protocol: TCP
  selector:
    app.kubernetes.io/name: log-archival
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/query-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-query-controller
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5004
    protocol: TCP
  selector:
    app.kubernetes.io/name: query-controller
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/raw-query-exec-controller/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-raw-query-exec-controller
  labels:
    helm.sh/chart: raw-query-exec-controller-0.1.1
    app.kubernetes.io/name: raw-query-exec-controller
    app.kubernetes.io/instance: archival
    release: archival
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
    - port: 8081
      targetPort: grpc
      protocol: TCP
      name: grpc
  selector:
    app.kubernetes.io/name: raw-query-exec-controller
    app.kubernetes.io/instance: archival
    release: archival
---
# Source: sf-archival/charts/spark-manager/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: archival-spark-manager
  namespace: kafka
spec:
  ports:
  - port: 80
    targetPort: 5002
    protocol: TCP
  selector:
    app.kubernetes.io/name: spark-manager
    app.kubernetes.io/instance: archival
---
# Source: sf-archival/charts/compaction-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-compaction-controller
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: compaction-controller
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: 2c5cdb7c3b641ae7c887e4c2b6de72eece68d73caf060f75b5e8a4560f3b1b66
      labels:
        app.kubernetes.io/name: compaction-controller
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: compaction-controller
          image: "snappyflowml/compaction-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 50m
              memory: 75Mi
            requests:
              cpu: 10m
              memory: 10Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-compaction-controller
---
# Source: sf-archival/charts/dataset-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-dataset-controller
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-controller
    helm.sh/chart: dataset-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: dataset-controller
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: 04a898e921789cc500e0d0ae5ee367de0d3d76b728691f4a5d6a4584f59f1789
      labels:
        app.kubernetes.io/name: dataset-controller
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: dataset-controller
          image: "snappyflowml/dataset-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 50m
              memory: 256Mi
            requests:
              cpu: 10m
              memory: 256Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-dataset-controller
---
# Source: sf-archival/charts/dataset-raw-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-dataset-raw-controller
  namespace: kafka
  labels:

    app.kubernetes.io/name: dataset-raw-controller
    helm.sh/chart: dataset-raw-controller-0.1.3
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.16.1"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: dataset-raw-controller
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: 087253aaa3d31d8d15d52f248b9de69bb500927ad493953445371935b68c880e
        prometheus.io/path: /metrics
        prometheus.io/port: "9000"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: dataset-raw-controller
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: dataset-raw-controller
          image: "snappyflowml/dataset-raw-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 250m
              memory: 786Mi
            requests:
              cpu: 100m
              memory: 512Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-dataset-raw-controller
---
# Source: sf-archival/charts/hive-and-hadoop/templates/hive-metastore-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-hive-metastore
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hive
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hive-metastore
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: a2684d0388e144b5fdc4ccccdb13cb4304d821f54b60225f42eedde205fb243e
        prometheus.io/scrape: "false"
      labels:
        app.kubernetes.io/name: hive-metastore
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'hive_metastore'; do sleep 3; done"
        - name: namenode-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-namenode-headless; do echo waiting for namenode; sleep 2; done"
        - name: datanode-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-datanode-headless; do echo waiting for datanode; sleep 2; done"
      containers:
        - name: metastore
          image: "snappyflowml/hive-2.3.8-and-hadoop-2.10.1:metastore-v1"
          imagePullPolicy: IfNotPresent
          ports:
            - name: metastore
              containerPort: 9083
              protocol: TCP
          envFrom:
            - configMapRef:
                name: archival-hive-and-hadoop-env
            - configMapRef:
                name: archival-cloud-env
          env:
            - name: HADOOP_HEAPSIZE
              value: "750"
          startupProbe:
            tcpSocket:
              port: metastore
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            tcpSocket:
              port: metastore
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 1Gi
---
# Source: sf-archival/charts/hive-and-hadoop/templates/hive-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-hive-server
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hive
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: hive-server
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: a2684d0388e144b5fdc4ccccdb13cb4304d821f54b60225f42eedde205fb243e
        prometheus.io/scrape: "false"
      labels:
        app.kubernetes.io/name: hive-server
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'hive_metastore'; do sleep 3; done"
        - name: namenode-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-namenode-headless; do echo waiting for namenode; sleep 2; done"
        - name: datanode-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-datanode-headless; do echo waiting for datanode; sleep 2; done"
        - name: metastore-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-hive-metastore; do echo waiting for metastore; sleep 2; done"
      containers:
        - name: server
          image: "snappyflowml/hive-2.3.8-and-hadoop-2.10.1:server-v1"
          imagePullPolicy: IfNotPresent
          ports:
            - name: server
              containerPort: 10000
              protocol: TCP
          envFrom:
            - configMapRef:
                name: archival-hive-and-hadoop-env
            - configMapRef:
                name: archival-cloud-env
          env:
            - name: HADOOP_HEAPSIZE
              value: "2000"
          startupProbe:
            tcpSocket:
              port: server
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            tcpSocket:
              port: server
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 3Gi
            requests:
              cpu: 300m
              memory: 3Gi
---
# Source: sf-archival/charts/ingest-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-ingest-controller
  namespace: kafka
  labels:
    app.kubernetes.io/name: ingest-controller
    helm.sh/chart: ingest-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingest-controller
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: 28cad07de2981539b9882b106933c8f63bae04fac148a0c3f11c29e1d0570ced
      labels:
        app.kubernetes.io/name: ingest-controller
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: ingest-controller
          image: "snappyflowml/ingest-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 50m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 10Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-ingest-controller
---
# Source: sf-archival/charts/log-archival/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-log-archival
  namespace: kafka
  labels:
    app.kubernetes.io/name: log-archival
    helm.sh/chart: log-archival-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: log-archival
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: 090a4f877f062c663c413ed5df02fbb7e0ddb8fff7ca3691a77e257b38cb6238
      labels:
        app.kubernetes.io/name: log-archival
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: log-archival
          image: "snappyflowml/log-archival:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 10Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-log-archival
---
# Source: sf-archival/charts/query-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-query-controller
  namespace: kafka
  labels:
    app.kubernetes.io/name: query-controller
    helm.sh/chart: query-controller-0.1.3
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: query-controller
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: c1f89ace1a990e9816deff05aa8555db94e24ff3f97ccb89c17bb1f73028f4be
      labels:
        app.kubernetes.io/name: query-controller
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
      containers:
        - name: query-controller
          image: "snappyflowml/query-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 512Mi
        - name: query-execution-controller
          image: "snappyflowml/query-execution-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: execution-controler-config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 512Mi
        - name: query-cleanup
          image: "snappyflowml/query-cleanup:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: cleanup-config-volume
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
            requests:
              cpu: 10m
              memory: 10Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-query-controller
        - name: execution-controler-config-volume
          configMap:
            name: archival-query-controller-execution-controller
        - name: cleanup-config-volume
          configMap:
            name: archival-query-controller-cleanup
---
# Source: sf-archival/charts/raw-query-exec-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-raw-query-exec-controller
  labels:
    helm.sh/chart: raw-query-exec-controller-0.1.1
    app.kubernetes.io/name: raw-query-exec-controller
    app.kubernetes.io/instance: archival
    release: archival
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: raw-query-exec-controller
      app.kubernetes.io/instance: archival
      release: archival
  template:
    metadata:
      labels:
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
        app.kubernetes.io/name: raw-query-exec-controller
        app.kubernetes.io/instance: archival
        release: archival
    spec:
      securityContext:
        {}
      containers:
        - name: raw-query-exec-controller
          image: "snappyflowml/raw-query-exec-controller:master-2-0-102"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 8081
              protocol: TCP
          volumeMounts:
            - name: config-volume
              mountPath: /etc/conf
          envFrom:
            - configMapRef:
                name: archival-cloud-env
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 50m
              memory: 256Mi
      volumes:
        - name: config-volume
          configMap:
            name: archival-raw-query-exec-controller
---
# Source: sf-archival/charts/spark-manager/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: archival-spark-manager
  namespace: kafka
  labels:
    app.kubernetes.io/name: spark-manager
    helm.sh/chart: spark-manager-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: spark-manager
      app.kubernetes.io/instance: archival
  template:
    metadata:
      annotations:
        checksum/config: fd53a6177bc951cccbd1c97356a3d985ba7e2c851a5754bde60cd55942e1d1de
      labels:
        app.kubernetes.io/name: spark-manager
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
        - name: cb-spark-event
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          image: "snappyflowml/cb-spark-event:master-2-0-102"
          imagePullPolicy: IfNotPresent
          args:
            - sparkhs/spark-hs
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'host':5432/'new_archival'; do sleep 3; done"
        - name: compaction-infrastructure
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          image: snappyflowml/compaction-infra:3
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume-compaction
            mountPath: /etc/conf
      containers:
        - name: monitor
          image: "snappyflowml/monitor:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume-monitor
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 10m
              memory: 10Mi
        - name: jobserver
          image: "snappyflowml/job-server:master-2-0-102"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume-jobserver
            mountPath: /etc/conf
          envFrom:
          - configMapRef:
              name: archival-cloud-env
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 50Mi
      volumes:
        - name: config-volume-monitor
          configMap:
            name: archival-spark-manager-monitor
        - name: config-volume-jobserver
          configMap:
            name: archival-spark-manager-jobserver
        - name: config-volume-compaction
          configMap:
            name: archival-spark-manager-infrastructure
---
# Source: sf-archival/charts/dataset-raw-controller/templates/hpa.yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: archival-dataset-raw-controller
  labels:
    app.kubernetes.io/name: dataset-raw-controller
    helm.sh/chart: dataset-raw-controller-0.1.3
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.16.1"
    app.kubernetes.io/managed-by: Helm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: archival-dataset-raw-controller
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        periodSeconds: 60
        value: 1
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Pods
        periodSeconds: 30
        value: 1
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Pods
      pods:
        metric:
          name: event_queue_handler_occupancy_percent
        target:
          type: AverageValue
          averageValue: 80
---
# Source: sf-archival/charts/hive-and-hadoop/templates/datanode-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: archival-hadoop-datanode
  namespace: kafka
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hadoop
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 
  selector:
    matchLabels:
      app.kubernetes.io/name: hadoop-datanode
      app.kubernetes.io/instance: archival
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: datanode
    spec:
      accessModes:
      - ReadWriteOnce
      
      resources:
        requests:
          storage: 10Gi
      volumeMode: Filesystem
  serviceName: archival-datanode-headless
  template:
    metadata:
      annotations:
        checksum/config: a2684d0388e144b5fdc4ccccdb13cb4304d821f54b60225f42eedde205fb243e
        prometheus.io/scrape: "false"
      labels:
        app.kubernetes.io/name: hadoop-datanode
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: namenode-ready
          image: busybox:1.28
          command:
            - 'sh'
            - '-c'
            - "until nslookup archival-namenode-headless; do echo waiting for namenode; sleep 2; done"
      containers:
        - name: datanode
          image: "snappyflowml/hadoop-2.10.1:datanode-v1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: datanode
            mountPath: /hadoop/dfs/data
          envFrom:
          - configMapRef:
              name: archival-hive-and-hadoop-env
          - configMapRef:
              name: archival-cloud-env
          env:
          - name: HADOOP_HEAPSIZE
            value: "750"
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 1Gi
          ports:
            - name: datanode
              containerPort: 50010
              protocol: TCP
          startupProbe:
            tcpSocket:
              port: datanode
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            tcpSocket:
              port: datanode
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 1Gi
---
# Source: sf-archival/charts/hive-and-hadoop/templates/namenode-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: archival-hadoop-namenode
  namespace: kafka
  labels:
    helm.sh/chart: hive-and-hadoop-0.2.1
    app.kubernetes.io/name: hadoop
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "2.3.8-and-2.10.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 
  selector:
    matchLabels:
      app.kubernetes.io/name: hadoop-namenode
      app.kubernetes.io/instance: archival
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: "namenode"
    spec:
      accessModes:
      - ReadWriteOnce
      
      resources:
        requests:
          storage: 10Gi
      volumeMode: Filesystem
  serviceName: archival-namenode-headless
  template:
    metadata:
      annotations:
        checksum/config: a2684d0388e144b5fdc4ccccdb13cb4304d821f54b60225f42eedde205fb243e
        prometheus.io/scrape: "false"
      labels:
        app.kubernetes.io/name: hadoop-namenode
        app.kubernetes.io/instance: archival
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      imagePullSecrets:
        - name: xxxx
      containers:
        - name: namenode
          image: "snappyflowml/hadoop-2.10.1:namenode-v1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: namenode
            mountPath: /hadoop/dfs/name
          envFrom:
          - configMapRef:
              name: archival-hive-and-hadoop-env
          - configMapRef:
              name: archival-cloud-env
          env:
          - name: HADOOP_HEAPSIZE
            value: "750"
          - name: CLUSTER_NAME
            value: archival
          ports:
            - name: namenode
              containerPort: 8020
              protocol: TCP
          startupProbe:
            tcpSocket:
              port: namenode
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            tcpSocket:
              port: namenode
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 1Gi
---
# Source: sf-archival/charts/compaction-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-create-compaction-candidates
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-compaction-controller; do echo waiting for archival-compaction-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-compaction-controller/compaction/create
          restartPolicy: Never
---
# Source: sf-archival/charts/compaction-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-queue-compaction-candidates
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-compaction-controller; do echo waiting for archival-compaction-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-compaction-controller/compaction/queue
          restartPolicy: Never
---
# Source: sf-archival/charts/compaction-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-process-stuck-compaction-candidates
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-compaction-controller; do echo waiting for archival-compaction-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-compaction-controller/compaction/process-stuck
          restartPolicy: Never
---
# Source: sf-archival/charts/compaction-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-run-compaction
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-compaction-controller; do echo waiting for archival-compaction-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-compaction-controller/compaction/start
          restartPolicy: Never
---
# Source: sf-archival/charts/compaction-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-partition-status-update
  namespace: kafka
  labels:
    app.kubernetes.io/name: compaction-controller
    helm.sh/chart: compaction-controller-0.1.1
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-compaction-controller; do echo waiting for archival-compaction-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-compaction-controller/compaction/update-partition-compaction-status
          restartPolicy: Never
---
# Source: sf-archival/charts/dataset-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-expire-partitions
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-controller
    helm.sh/chart: dataset-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "1 0 * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-dataset-controller; do echo waiting for archival-dataset-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X DELETE http://archival-dataset-controller/datasets/partitions/expired
          restartPolicy: Never
---
# Source: sf-archival/charts/dataset-controller/templates/cron.yaml
---

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-analyze-segments
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-controller
    helm.sh/chart: dataset-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "0 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-dataset-controller; do echo waiting for archival-dataset-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-dataset-controller/datasets/partitions/analyze-segments-and-store-boundary-times
          restartPolicy: Never
---
# Source: sf-archival/charts/dataset-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-mark-compaction-complete-partitions
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-controller
    helm.sh/chart: dataset-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "0 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-dataset-controller; do echo waiting for archival-dataset-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/busybox-curl:0.0.1
            imagePullPolicy: IfNotPresent
            args:
            - /bin/sh
            - -c
            - curl -X POST http://archival-dataset-controller/datasets/partitions/mark-compaction-complete-partitions
          restartPolicy: Never
---
# Source: sf-archival/charts/dataset-raw-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-process-raw-data
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-raw-controller
    helm.sh/chart: dataset-raw-controller-0.1.3
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.16.1"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "0 1 * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: dataset-raw-controller-init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-dataset-raw-controller; do echo waiting for archival-dataset-raw-controller; sleep 5; done"]
          - name: ingest-controller-init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-ingest-controller; do echo waiting for archival-dataset-raw-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/drc-process-raw-data-job:4
            imagePullPolicy: IfNotPresent
            command: ["/bin/sh","-c", "/app/drc-process-raw-data-job -dataset-raw-controller-url http://archival-dataset-raw-controller -parallelism-factor 1"]
          restartPolicy: Never
---
# Source: sf-archival/charts/dataset-raw-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-delete-raw-data-post-compaction
  namespace: kafka
  labels:
    app.kubernetes.io/name: dataset-raw-controller
    helm.sh/chart: dataset-raw-controller-0.1.3
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.16.1"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "0 5 * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          initContainers:
          - name: dataset-raw-controller-init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-dataset-raw-controller; do echo waiting for archival-dataset-raw-controller; sleep 5; done"]
          - name: ingest-controller-init
            image: busybox:1.28
            command: ['sh', '-c', "until nslookup archival-ingest-controller; do echo waiting for archival-dataset-raw-controller; sleep 5; done"]
          containers:
          - name: main
            image: snappyflowml/drc-delete-raw-data-job:1
            imagePullPolicy: IfNotPresent
            command: ["/bin/sh","-c", "/app/drc-delete-raw-data-job -dataset-raw-controller-url http://archival-dataset-raw-controller -parallelism-factor 1"]
          restartPolicy: Never
---
# Source: sf-archival/charts/ingest-controller/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: archival-sinks-monitor
  namespace: kafka
  labels:
    app.kubernetes.io/name: ingest-controller
    helm.sh/chart: ingest-controller-0.1.2
    app.kubernetes.io/instance: archival
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/10 * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: archival
        spec:
          containers:
          - name: main
            image: snappyflowml/sink-monitor-consumer-groups:7
            imagePullPolicy: IfNotPresent
            command: ["bash"]
            args: ["-c", "python app.py connect-s3"]
            volumeMounts:
              - name: archival-ingest-controller
                mountPath: "/etc/config"
          restartPolicy: Never
          volumes:
            - name: archival-ingest-controller
              configMap:
                name: archival-ingest-controller
---
# Source: sf-archival/templates/system-migration.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: archival-system-migration
  namespace: kafka
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "archival"
    helm.sh/chart: "sf-archival-2.0.88"
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    metadata:
      name: archival-system-migraton
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "archival"
        helm.sh/chart: "sf-archival-2.0.88"
        release: archival
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: archival
    spec:
      restartPolicy: Never
      containers:
      - name: main
        image: "snappyflowml/archival-system-migration:master-2-0-102"
        imagePullPolicy: Always
        volumeMounts:
          - name: archival-system-migration-config
            mountPath: "/etc/conf"
            readOnly: true
        envFrom:
          - configMapRef:
              name: archival-cloud-env
      volumes:
        - name: archival-system-migration-config
          configMap:
            name: archival-system-migration-config
